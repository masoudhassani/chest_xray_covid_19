{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# LOGGING CONFIG ##############################\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "using the chest x-ray dataset from https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/\n",
    "check if the chest x-ray dataset exists\n",
    "unzip the chest x-ray dataset if it exists\n",
    "'''\n",
    "if not os.path.exists('chest_xray'):\n",
    "    if os.path.exists('chest-xray-pneumonia.zip'):\n",
    "        logging.info('unzipping the dataset file')\n",
    "        os.system('unzip chest-xray-pneumonia.zip')\n",
    "        logging.info('unzipping is done')\n",
    "        os.system('rm chest-xray-pneumonia.zip')\n",
    "    else:\n",
    "        logging.warning('please download the dataset from https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-05-13 21:50:02 INFO     reading images for category NORMAL\n2020-05-13 21:50:28 INFO     reading images for category PNEUMONIA\n2020-05-13 21:50:51 INFO     reading images done\n"
    }
   ],
   "source": [
    "def create_training_data(dir, data_type, main, sub, size):\n",
    "    training_data = []\n",
    "    occurrence = [0, 0, 0]\n",
    "    for cat in main:\n",
    "        logging.info('reading images for category {}'.format(cat))\n",
    "        path =  os.path.join(dir, data_type, cat)\n",
    "        label = main.index(cat)\n",
    "        for img in os.listdir(path):\n",
    "            # read and resize image\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (size, size))\n",
    "\n",
    "                # assign label for virus and bateria\n",
    "                if label != 0:\n",
    "                    if sub[0] in img:\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = 2\n",
    "\n",
    "                # append to training data\n",
    "                occurrence[label] += 1\n",
    "                training_data.append([img_array, label])\n",
    "\n",
    "            except:\n",
    "                logging.warn('error reading {}'.format(img))\n",
    "    \n",
    "    logging.info('reading images done')\n",
    "    return training_data, occurrence\n",
    "\n",
    "            \n",
    "directory = 'chest_xray'\n",
    "data_type = 'train'   # choose between train and test\n",
    "main_category = ['NORMAL', 'PNEUMONIA']\n",
    "sub_category = ['bacteria', 'virus']\n",
    "image_size = 200\n",
    "\n",
    "# create training data with labels: 0:normal, 1:bacterial 2:viral\n",
    "training_data, occurrence = create_training_data(directory, data_type, main_category, sub_category, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Normal: 1341 Bacterial: 2530 Viral: 1345\nNormal: 0.2570935582822086 Bacterial: 0.48504601226993865 Viral: 0.25786042944785276\n"
    }
   ],
   "source": [
    "# number of images with labels 0, 1 and 2\n",
    "print('Normal:', occurrence[0], 'Bacterial:', occurrence[1], 'Viral:', occurrence[2])\n",
    "# weight is labels based on their occurence\n",
    "weight = [float(i)/sum(occurrence) for i in occurrence]\n",
    "print('Normal:', weight[0], 'Bacterial:', weight[1], 'Viral:', weight[2])\n",
    "\n",
    "# shuffle the training data otherwise the neural network model will be inefficient\n",
    "import random \n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and labels\n",
    "X = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to convert a list to a numpy array that is understandable for tensorflow\n",
    "# -1 means everything in the list, 1 is because the image is gray scale\n",
    "X = np.array(X).reshape(-1, image_size, image_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training data\n",
    "# you need at least 8GB of ram for this\n",
    "import pickle\n",
    "pickle_out = open('trainings/X.pickle', 'wb')\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('trainings/y.pickle', 'wb')\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to create neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X and y in case we want to re run from here\n",
    "import pickle\n",
    "X = pickle.load(open('trainings/X.pickle', 'rb'))\n",
    "y = pickle.load(open('trainings/y.pickle', 'rb'))\n",
    "\n",
    "# in case of gray scale image data, we normalize it \n",
    "X = X/255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 50, 50, 32)        544       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 50, 50, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 24, 24, 32)        16416     \n_________________________________________________________________\nactivation_5 (Activation)    (None, 24, 24, 32)        0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 18432)             0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                1179712   \n_________________________________________________________________\ndense_5 (Dense)              (None, 3)                 195       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 1,196,867\nTrainable params: 1,196,867\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "# create the model \n",
    "model = Sequential() \n",
    "\n",
    "# layer 1\n",
    "model.add(Conv2D(32, (4,4), strides=(4, 4), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# layer 2\n",
    "model.add(Conv2D(32, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# layer 3\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "# output layer, we use Dense(3) to have 3 labels 0,1,2\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 4694 samples, validate on 522 samples\nEpoch 1/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.4307 - acc: 0.8130 - val_loss: 0.4861 - val_acc: 0.7835\nEpoch 2/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.4087 - acc: 0.8179 - val_loss: 0.4945 - val_acc: 0.7739\nEpoch 3/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.3967 - acc: 0.8304 - val_loss: 0.4727 - val_acc: 0.7893\nEpoch 4/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.3608 - acc: 0.8426 - val_loss: 0.4743 - val_acc: 0.7969\nEpoch 5/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.3427 - acc: 0.8509 - val_loss: 0.4965 - val_acc: 0.7778\nEpoch 6/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.3208 - acc: 0.8626 - val_loss: 0.5403 - val_acc: 0.7567\nEpoch 7/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.2940 - acc: 0.8709 - val_loss: 0.5169 - val_acc: 0.7931\nEpoch 8/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.2783 - acc: 0.8813 - val_loss: 0.5016 - val_acc: 0.7893\nEpoch 9/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.2380 - acc: 0.9001 - val_loss: 0.5255 - val_acc: 0.7835\nEpoch 10/40\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.2332 - acc: 0.9090 - val_loss: 0.5565 - val_acc: 0.7663\nEpoch 11/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.2150 - acc: 0.9118 - val_loss: 0.6224 - val_acc: 0.7931\nEpoch 12/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.1749 - acc: 0.9327 - val_loss: 0.6171 - val_acc: 0.7759\nEpoch 13/40\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.1497 - acc: 0.9465 - val_loss: 0.6110 - val_acc: 0.7605\nEpoch 14/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.1280 - acc: 0.9546 - val_loss: 0.6960 - val_acc: 0.7490\nEpoch 15/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.1093 - acc: 0.9621 - val_loss: 0.7389 - val_acc: 0.7835\nEpoch 16/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.1017 - acc: 0.9640 - val_loss: 0.7623 - val_acc: 0.7644\nEpoch 17/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0729 - acc: 0.9764 - val_loss: 0.7683 - val_acc: 0.7797\nEpoch 18/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0724 - acc: 0.9778 - val_loss: 0.9467 - val_acc: 0.7644\nEpoch 19/40\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.0427 - acc: 0.9911 - val_loss: 0.8910 - val_acc: 0.7835\nEpoch 20/40\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0398 - acc: 0.9913 - val_loss: 0.9362 - val_acc: 0.7663\nEpoch 21/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0413 - acc: 0.9893 - val_loss: 1.0928 - val_acc: 0.7682\nEpoch 22/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0361 - acc: 0.9915 - val_loss: 1.1497 - val_acc: 0.7835\nEpoch 23/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.2423 - acc: 0.9031 - val_loss: 0.6024 - val_acc: 0.7720\nEpoch 24/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.1577 - acc: 0.9410 - val_loss: 0.7980 - val_acc: 0.7510\nEpoch 25/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0871 - acc: 0.9715 - val_loss: 0.8381 - val_acc: 0.7739\nEpoch 26/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0491 - acc: 0.9862 - val_loss: 1.0129 - val_acc: 0.7663\nEpoch 27/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0317 - acc: 0.9960 - val_loss: 0.8554 - val_acc: 0.7605\nEpoch 28/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0225 - acc: 0.9968 - val_loss: 1.1927 - val_acc: 0.7663\nEpoch 29/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0146 - acc: 0.9994 - val_loss: 1.1393 - val_acc: 0.7548\nEpoch 30/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0096 - acc: 0.9994 - val_loss: 1.2299 - val_acc: 0.7682\nEpoch 31/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0068 - acc: 1.0000 - val_loss: 1.3139 - val_acc: 0.7625\nEpoch 32/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0049 - acc: 1.0000 - val_loss: 1.3767 - val_acc: 0.7625\nEpoch 33/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 1.3962 - val_acc: 0.7663\nEpoch 34/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 1.4155 - val_acc: 0.7605\nEpoch 35/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 1.4745 - val_acc: 0.7586\nEpoch 36/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 1.5028 - val_acc: 0.7644\nEpoch 37/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 1.5514 - val_acc: 0.7663\nEpoch 38/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 1.5419 - val_acc: 0.7567\nEpoch 39/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 1.5708 - val_acc: 0.7605\nEpoch 40/40\n4694/4694 [==============================] - 8s 2ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 1.5918 - val_acc: 0.7644\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fd4a7724898>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=100, epochs=40, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"trainings/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bittfvenvc7eebf1f4b234859800ec2a4f8559337",
   "display_name": "Python 3.6.9 64-bit ('tf': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}