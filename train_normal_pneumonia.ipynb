{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# LOGGING CONFIG ##############################\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "using the chest x-ray dataset from https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/\n",
    "check if the chest x-ray dataset exists\n",
    "unzip the chest x-ray dataset if it exists\n",
    "'''\n",
    "if not os.path.exists('chest_xray'):\n",
    "    if os.path.exists('chest-xray-pneumonia.zip'):\n",
    "        logging.info('unzipping the dataset file')\n",
    "        os.system('unzip chest-xray-pneumonia.zip')\n",
    "        logging.info('unzipping is done')\n",
    "        os.system('rm chest-xray-pneumonia.zip')\n",
    "    else:\n",
    "        logging.warning('please download the dataset from https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-05-14 15:30:03 INFO     reading images for category NORMAL\n2020-05-14 15:30:33 INFO     reading images for category PNEUMONIA\n2020-05-14 15:30:58 INFO     reading images done\n"
    }
   ],
   "source": [
    "def create_data_from_image(dir, data_type, main, size):\n",
    "    training_data = []\n",
    "    occurrence = [0, 0, 0]\n",
    "    for cat in main:\n",
    "        logging.info('reading images for category {}'.format(cat))\n",
    "        path =  os.path.join(dir, data_type, cat)\n",
    "        label = main.index(cat)\n",
    "        for img in os.listdir(path):\n",
    "            # read and resize image\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (size, size))\n",
    "\n",
    "                # append to training data\n",
    "                occurrence[label] += 1\n",
    "                training_data.append([img_array, label])\n",
    "\n",
    "            except:\n",
    "                logging.warn('error reading {}'.format(img))\n",
    "    \n",
    "    logging.info('reading images done')\n",
    "    return training_data, occurrence\n",
    "\n",
    "            \n",
    "directory = 'chest_xray'\n",
    "data_type = 'train'   # choose between train and test\n",
    "main_category = ['NORMAL', 'PNEUMONIA']\n",
    "image_size = 200\n",
    "\n",
    "# create training data with labels: 0:normal, 1:bacterial 2:viral\n",
    "training_data, occurrence = create_data_from_image(directory, data_type, main_category, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Normal: 1341 Pneumonia: 3875\nNormal: 0.2570935582822086 Pneumonia: 0.7429064417177914\n"
    }
   ],
   "source": [
    "# number of images with labels 0, 1\n",
    "print('Normal:', occurrence[0], 'Pneumonia:', occurrence[1])\n",
    "# weight is labels based on their occurence\n",
    "weight = [float(i)/sum(occurrence) for i in occurrence]\n",
    "print('Normal:', weight[0], 'Pneumonia:', weight[1])\n",
    "class_weight = {0: weight[0],\n",
    "                1: weight[1]}\n",
    "\n",
    "# shuffle the training data otherwise the neural network model will be inefficient\n",
    "import random \n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and labels\n",
    "X = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to convert a list to a numpy array that is understandable for tensorflow\n",
    "# -1 means everything in the list, 1 is because the image is gray scale\n",
    "X = np.array(X).reshape(-1, image_size, image_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training data\n",
    "# you need at least 8GB of ram for this\n",
    "import pickle\n",
    "pickle_out = open('trainings/X_2labels.pickle', 'wb')\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('trainings/y_2labels.pickle', 'wb')\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to create neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to read X and y in case we want to re run from here\n",
    "# this avoids the need to re-read all images\n",
    "# import pickle\n",
    "# X = pickle.load(open('trainings/X_2labels.pickle', 'rb'))\n",
    "# y = pickle.load(open('trainings/y_2labels.pickle', 'rb'))\n",
    "\n",
    "# in case of gray scale image data, we normalize it \n",
    "X = X/255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "NAME = 'xray-2_labels-4_layers-2x32-{}'.format(int(time.time()))\n",
    "# tensorboard = TensorBoard(log_dir='trainings/{}'.format(NAME))\n",
    "# os.system(\"tensorboard --logdir='trainin/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_8 (Conv2D)            (None, 50, 50, 32)        544       \n_________________________________________________________________\nactivation_16 (Activation)   (None, 50, 50, 32)        0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 24, 24, 32)        16416     \n_________________________________________________________________\nactivation_17 (Activation)   (None, 24, 24, 32)        0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 18432)             0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 64)                1179712   \n_________________________________________________________________\nactivation_18 (Activation)   (None, 64)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 2)                 130       \n_________________________________________________________________\nactivation_19 (Activation)   (None, 2)                 0         \n=================================================================\nTotal params: 1,196,802\nTrainable params: 1,196,802\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "# create the model \n",
    "model = Sequential() \n",
    "\n",
    "# layer 1\n",
    "model.add(Conv2D(32, (4,4), strides=(4, 4), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# layer 2\n",
    "model.add(Conv2D(32, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# layer 3\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# output layer, we use Dense(3) to have 3 labels 0,1,2\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 4694 samples, validate on 522 samples\nEpoch 1/20\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.1737 - acc: 0.7857 - val_loss: 0.0876 - val_acc: 0.9502\nEpoch 2/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0688 - acc: 0.9188 - val_loss: 0.0381 - val_acc: 0.9693\nEpoch 3/20\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.0489 - acc: 0.9495 - val_loss: 0.0339 - val_acc: 0.9502\nEpoch 4/20\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.0440 - acc: 0.9550 - val_loss: 0.0301 - val_acc: 0.9636\nEpoch 5/20\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.0420 - acc: 0.9578 - val_loss: 0.0322 - val_acc: 0.9579\nEpoch 6/20\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.0325 - acc: 0.9646 - val_loss: 0.0248 - val_acc: 0.9770\nEpoch 7/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0287 - acc: 0.9683 - val_loss: 0.0265 - val_acc: 0.9693\nEpoch 8/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0267 - acc: 0.9712 - val_loss: 0.0465 - val_acc: 0.9713\nEpoch 9/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0269 - acc: 0.9715 - val_loss: 0.0284 - val_acc: 0.9789\nEpoch 10/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0176 - acc: 0.9838 - val_loss: 0.0307 - val_acc: 0.9789\nEpoch 11/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0158 - acc: 0.9859 - val_loss: 0.0403 - val_acc: 0.9751\nEpoch 12/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0258 - val_acc: 0.9732\nEpoch 13/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0168 - acc: 0.9842 - val_loss: 0.0210 - val_acc: 0.9789\nEpoch 14/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0095 - acc: 0.9906 - val_loss: 0.0279 - val_acc: 0.9674\nEpoch 15/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0143 - acc: 0.9838 - val_loss: 0.0299 - val_acc: 0.9789\nEpoch 16/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0119 - acc: 0.9883 - val_loss: 0.0310 - val_acc: 0.9808\nEpoch 17/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0067 - acc: 0.9934 - val_loss: 0.0407 - val_acc: 0.9789\nEpoch 18/20\n4694/4694 [==============================] - 10s 2ms/sample - loss: 0.0063 - acc: 0.9951 - val_loss: 0.0332 - val_acc: 0.9808\nEpoch 19/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0059 - acc: 0.9955 - val_loss: 0.0277 - val_acc: 0.9770\nEpoch 20/20\n4694/4694 [==============================] - 9s 2ms/sample - loss: 0.0065 - acc: 0.9938 - val_loss: 0.0278 - val_acc: 0.9770\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f799c2f20b8>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=100, epochs=20, class_weight=class_weight, validation_split=0.1)\n",
    "# model.fit(X, y, batch_size=100, epochs=20, class_weight=class_weight, validation_split=0.1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the network weights\n",
    "model.save_weights(\"trainings/{}.h5\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bittfvenvc7eebf1f4b234859800ec2a4f8559337",
   "display_name": "Python 3.6.9 64-bit ('tf': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}